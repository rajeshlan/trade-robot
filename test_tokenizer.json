"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 5000, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": null, \"document_count\": 1, \"word_counts\": \"{\\\"this\\\": 1, \\\"is\\\": 1, \\\"a\\\": 1, \\\"test\\\": 1, \\\"tweet\\\": 1}\", \"word_docs\": \"{\\\"a\\\": 1, \\\"tweet\\\": 1, \\\"test\\\": 1, \\\"is\\\": 1, \\\"this\\\": 1}\", \"index_docs\": \"{\\\"3\\\": 1, \\\"5\\\": 1, \\\"4\\\": 1, \\\"2\\\": 1, \\\"1\\\": 1}\", \"index_word\": \"{\\\"1\\\": \\\"this\\\", \\\"2\\\": \\\"is\\\", \\\"3\\\": \\\"a\\\", \\\"4\\\": \\\"test\\\", \\\"5\\\": \\\"tweet\\\"}\", \"word_index\": \"{\\\"this\\\": 1, \\\"is\\\": 2, \\\"a\\\": 3, \\\"test\\\": 4, \\\"tweet\\\": 5}\"}}"